knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(eval = TRUE)
library(tidyverse) # for data wrangling
library(kableExtra) # for table outputs
library(wiscR) # some plotting functions
library(gridExtra)
tmp <- read.csv("../data/data.csv", row.names = 1)
```{r full-model-anove, message=F, warning=F}
tmp <- read.csv("../data/data.csv", row.names = 1)
colnames(tmp)[1:7] <- LETTERS[1:7]
df <- merge(x = tmp, y = df.design, by = LETTERS[1:7], all = TRUE, sort = FALSE)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(eval = TRUE)
library(tidyverse) # for data wrangling
library(kableExtra) # for table outputs
library(wiscR) # some plotting functions
library(gridExtra)
add_effect <- function(X, effect) {
# X is a matrix (likely made by `generate_initial_design`)
# effect is an effect represented by string of letters (e.g. "AB" or "ADE")
# OR effect is a an integer
factors <- colnames(X)
if (typeof(effect) == "double"){
combos <- t(combn(factors, 2))
all_effects <- apply(combos, 1, paste0, collapse = "")
} else {
all_effects <- effect
}
for (string in all_effects){
v <- strsplit(string, split = "")[[1]]
X <- cbind(X, apply(X[, v], 1, prod))
colnames(X)[ncol(X)] <- string
}
return(X)
}
get_confounded <- function(I){
# I is identifying relation (I = "ABCD")
# Returns all confounded variables as data.frame
I.split <- (strsplit(I, ""))[[1]]
# total number of combinations is 2^n.
# Subtract one since don't care about n choose 0
# Subtract another 1 since don't need n choose n
N <- 2^(length(I.split)) - 2
df <- data.frame(lower.confounded = rep("", N),
higher.confounded = rep("", N),
stringsAsFactors = F)
ix <- 1
for (i in 1:(length(I.split)-1)) {
combos <- apply(t(combn(I.split, i)), 1, paste0, collapse = "")
for (string in combos) {
tmp <- c(string, multiply_effects(c(I, string)))
df[ix, ] <-  tmp[order(nchar(tmp), tmp)]
ix <- ix + 1
}
}
return(distinct(df))
}
get_all_confounded <- function(multiple_Is){
N <- length(multiple_Is)
con.df <- get_confounded(multiple_Is[1])
for (i in 1:N){
combos <- t(combn(multiple_Is, i))
for (j in 1:nrow(combos)){
vec <- ifelse(i %in% c(1,N), combos[j, ], multiply_effects(combos[j, ]))
tmp.df <- get_confounded(vec)
con.df <- rbind(con.df, tmp.df)
}
}
return(con.df %>% distinct())
}
Lenths_method <- function(effects, alpha) {
#effects: A vector or (preferably) a named list
#alpha: level of test
abs_effects <-  abs(effects)
g <- length(abs_effects)
s0 <- 1.5 * median(abs_effects)
PSE <- 1.5 * median(abs_effects[abs_effects < 2.5 * s0])
nu <- g / 3
gamma <- 0.5 * (1 - (1 - alpha)^(1/g))
critical <- qt(p = gamma, df = nu, lower.tail = F) * PSE
ix <- abs_effects > critical
return(list(sig.effects=effects[ix], critical=critical) )
}
# Dataset from slide 178 as gutcheck
# demo <- c(A=23, B=-5, C=1.5, AB=1.5, AC=10, BC=0, ABC=0.5)
# Lenths_method(demo, 0.1)
Dongs_method <- function(effects, alpha) {
g <- length(effects)
s <- 1.5 * median(abs(effects))
for (i in 1:2){
ix <- abs(effects) <= 2.5*s
m <- sum(ix)
s_sq <- sum(effects[ix]^2) / m
s <- sqrt(s_sq)
}
gamma = 0.5 * (1 - (1 - alpha)^(1/g))
critical <- qt(p=gamma, df=m, lower.tail = F) * s
sig.ix <- abs(effects) > critical
return(list(sig.effects=effects[sig.ix], critical=critical) )
}
# demo <- c(A=23, B=-5, C=1.5, AB=1.5, AC=10, BC=0, ABC=0.5)
# Dongs_method(demo, 0.1)
transform_BoxCox <- function(y, lambda) {
# y is response vector
# lambda is a Box-Cox parameter
# Following chunk borrows heavily from Prof. Loh's code
gm <- exp(mean(log(y)))
if (lambda == 0){
y.out <- gm*log(y)
} else {
y.out <- (y^lambda - 1) / (lambda * gm^(lambda-1))
}
return(y.out)
}
get_best_lambda <- function(df, lower, upper){
# df must have a capital Y response column
# lower and upper are search space for lambda
lambda.range <- seq(from=lower,to=upper,length.out=50)
df.copy <- df
y0 <- pull(df, Y)
SSR <- NULL
for (lambda in lambda.range){
df.copy$Y <- transform_BoxCox(y0, lambda)
fit <- lm(Y ~ ., df.copy)
SSR <- c(SSR, sum(fit$resid %*% fit$resid))
}
# Lambda that minimized sum of square residuals
lambda.optimal <- round(lambda.range[which(SSR==min(SSR))], 2)
# Return a data frame with transformed Y vector
df.transformed <- df %>% mutate(Y = transform_BoxCox(Y, lambda.optimal))
return(list(lambda.range=lambda.range, SSR=SSR, lambda.optimal=lambda.optimal,
df.transformed=df.transformed))
}
generate_initial_design <- function(p){
# Creates an X matrix with p columns. Each column is composed of 1s and -1s.
# The patterns of + and - are different in each column. For example:
# Col 1: + - + - + -...
# Col 2: + + - - + +...
# p is the number of factors and is only required (accepted) input.
runs <- 2^p
X <- matrix(rep(0, runs*p), nrow = runs)
colnames(X) <- LETTERS[seq( from = 1, to = p )] # assign column names from alphabet
for (jj in 1:p){
pattern_multiplier <- 2^(jj - 1)
string_multiplier <- 2^(p-jj)
ix <- c(rep(1, pattern_multiplier), rep(-1, pattern_multiplier))
X[,jj] <- rep(ix, string_multiplier)
}
return(X)
}
multiply_effects <- function(str_vec) {
# helper function, multiply letter strings
# E.g. multiply_effects("ABC", "CDE") --> "ABDE"
N <- length(str_vec)
while (N > 2) {
str_vec <- c(multiply_effects(str_vec[1:2]), str_vec[3:N])
N <- length(str_vec)
}
counts <- table(strsplit(paste0(str_vec[1],str_vec[2]), ""))
# If the number of occurrences is odd, that letter stays (was not canceled)
reduced <- paste0(names(counts[counts %% 2 == 1]), collapse="")
return(reduced)
}
# 2^(7-2) means 5 variables in standard order + 2 "generated" using
# Box, Hunter, Hunter
X <- generate_initial_design(5)
generator1 <- "ABCD" # = F -> I = ABCDF
generator2 <- "ABDE" # = G -> I = ABDEG
X <- add_effect(X, generator1)
X <- add_effect(X, generator2)
colnames(X)[6:7] <- c("F", "G")
block1 <- "CE"
block2 <- "CF"
X <- add_effect(X, block1)
X <- add_effect(X, block2)
colnames(X)[8:9] <- c("BlockX", "BlockY")
# Change block variables from BlockX BlockY to 4 named blocks
df.design <- X %>%
as.data.frame(row.names = F) %>%
mutate(tmp = paste0(BlockX, BlockY)) %>%
mutate(Block=recode_factor(tmp, `-1-1`="B1", `-11`="B2", `1-1`="B3", `11`="B4")) %>%
select(-c(BlockX, BlockY, tmp)) %>%
mutate(Run = 1:32) %>%
relocate(Run)
# Print table
t1 <- df.design[1:16, ]
t2 <- df.design[17:32,]
knitr::kable(list(t1, t2), booktabs = T, valign = 't',row.names = F,
caption = '32 Runs in standard order with blocks.') %>%
kable_classic()
tmp <- read.csv("../data/data.csv", row.names = 1)
colnames(tmp)[1:7] <- LETTERS[1:7]
df <- merge(x = tmp, y = df.design, by = LETTERS[1:7], all = TRUE, sort = FALSE)
contrasts(df$Block) <- matrix(c(1,0,0, 0,1,0, 0,0,1, 0,0,0), nrow = 4, byrow = T)
estimable_effects <- c("AB", "AC", "AD", "AE", "AF", "AG",
"BC", "BD", "BE", "BF", "BG",
"CD", "DE", "DF", "DG") # 15 estimable 2fi
for (ff in estimable_effects){
df <- add_effect(df, ff)
}
v <- c(LETTERS[1:7], estimable_effects, "Block", "Y")
df <- df %>%  select(v)
model.full <- lm(Y ~ . , data = df)
effects <- model.full$coefficients[-1]*2
tmp <- data.frame(Effect = names(effects),
Estimate = round(unname(effects),2))
t1 <- tmp[1:7, ]
t2 <- tmp[8:12,]
t3 <- tmp[13:17, ]
t4 <- tmp[18:22, ]
knitr::kable(list(t1,t2,t3,t4), booktabs = T, valign = 't',row.names = F,
caption = 'Estimated main and non-confounded two-factor interactions.') %>%
kable_classic()
Lenths_method(effects, 0.10)
Dongs_method(effects, 0.10)
model.simplified <- lm(Y ~ A + D + G, data = df)
anova(model.simplified)
# Subset a data frame for box-cox
df.input <- df %>% select(c(A,D,G, Y))
output <- get_best_lambda(df.input, 0, 1.5)
# Data wrangling for plotting
tmp <- data.frame(lambda = output$lambda.range, SSR = output$SSR)
p1 <- tmp %>%
ggplot(aes(x = lambda, y = SSR)) +
geom_point() +
geom_line() +
theme_minimal() +
xlab(expression(lambda)) +
ylab("Sum of squared residuals") +
geom_vline(xintercept = output$lambda.optimal, color = "blue") +
ggtitle("Finding optimal [lambda]")
p1
fit <- lm(Y ~ A+D+G, data = output$df.transformed)
anova(fit)
Lenths_method(2*fit$coefficients, 0.1)
Dongs_method(2*fit$coefficients, 0.1)
p2 <- data.frame(fitted = fit$fitted.values, residual=fit$residuals) %>%
ggplot(aes(x=fitted, y=residual)) +
geom_point() +
theme_minimal() +
xlab("Fitted response") +
ylab("Residual") +
ggtitle("Residuals compared \n to fitted values")
p3 <- wiscR::qqplot(fit$residuals) + theme_minimal() +
xlab("Theoretical quantile") +
ylab("Sample quantile") +
ggtitle("QQ Normal \n of residuals \n after Box-Cox transform")
grid.arrange(grobs=list(p1,p2,p3), ncol = 3)
X2 <- generate_initial_design(3) %>% as.data.frame()
row.names(X2) <- 1:8
set.seed(919)
ix <- sample(1:8)
X2 <- X2[ix, ]
write.csv(X2, "../design-matrices/design-followup.csv")
df.2 <- read.csv("../data/data-followup.csv", row.names = 1)
mod.2 <- lm(Y ~.^2, data=df.2)
eff.2 <- 2 * mod.2$coefficients
Lenths_method(eff.2, 0.2)
set.seed(919)
block.order <- sample(unique(df.design$Block))
R <- matrix(nrow = 4, ncol = 8)
for (b in 1:4){
R[b,] <- df.design %>%
filter(Block == block.order[b]) %>%
pull(Run) %>%
sample()
}
row.names(R) <- block.order
colnames(R) <- 1:8
R %>%
kable(valign = 'c', caption = 'Blocks and runs randomized', booktabs=T) %>%
kable_classic()  %>%
kable_styling(bootstrap_options = "striped")
Lenths_method(effects, 0.10)
Dongs_method(effects, 0.10)
effects
model.reduced <- lm(Y ~ A + D + G, data = df)
p1
p1
p1 <- qqplot(model.reduced$residuals)
p1 <- qqplot(model.reduced$residuals)
p1
model.reduced <- lm(Y ~ A + D + G, data = df)
p1 <- qqplot(model.reduced$residuals) +
xlab("Theoretical") +
ylab("Residual") +
ggtitle("QQ normal plot of reduced model residuals") +
theme_minimal()
p1
p2 <- data.frame(fitted = model.reduced$fitted.values, residual= model.reduced$residuals) %>%
ggplot(aes(x=fitted, y=residual)) +
geom_point() +
theme_minimal() +
xlab("Fitted response") +
ylab("Residual") +
ggtitle("Residuals compared \n to fitted values")
p2
model.reduced <- lm(Y ~ A + D + G + AE, data = df)
model.reduced <- lm(Y ~ A + D + G + A:E, data = df)
p1 <- qqplot(model.reduced$residuals) +
xlab("Theoretical") +
ylab("Residual") +
ggtitle("QQ normal plot of reduced model residuals") +
theme_minimal()
p2 <- data.frame(fitted = model.reduced$fitted.values, residual= model.reduced$residuals) %>%
ggplot(aes(x=fitted, y=residual)) +
geom_point() +
theme_minimal() +
xlab("Fitted response") +
ylab("Residual") +
ggtitle("Residuals compared \n to fitted values")
p2
model.reduced
grid.arrange(list(p1,p2))
grid.arrange(grobs=list(p1,p2))
grid.arrange(grobs=list(p1,p2), ncol=2)
```{r model-reduced, fig.height=3}
model.reduced <- lm(Y ~ A + D + G + A:E, data = df)
p1 <- qqplot(model.reduced$residuals) +
xlab("Theoretical") +
ylab("Residual") +
ggtitle("QQ normal plot of reduced model residuals") +
theme_minimal()
p2 <- data.frame(fitted = model.reduced$fitted.values, residual= model.reduced$residuals) %>%
ggplot(aes(x=fitted, y=residual)) +
geom_point() +
theme_minimal() +
xlab("Fitted") +
ylab("Residual") +
ggtitle("Residuals compared to fitted values")
grid.arrange(grobs=list(p1,p2), ncol=2)
model.followup <- lm(Y ~ A+D+G, data = output$df.transformed)
anova(fit)
Lenths_method(2*fit$coefficients[-1], 0.1)
Dongs_method(2*fit$coefficients[-1], 0.1)
Lenths_method(2*fit$coefficients[-1], 0.1)
Dongs_method(2*fit$coefficients[-1], 0.1)
anova(fit)
summary(model.followup)
X2 <- generate_initial_design(3) %>% as.data.frame()
row.names(X2) <- 1:8
set.seed(919)
ix <- sample(1:8)
X2 <- X2[ix, ]
write.csv(X2, "../design-matrices/design-followup.csv")
df.2 <- read.csv("../data/data-followup.csv", row.names = 1)
mod.2 <- lm(Y ~.^2, data=df.2)
eff.2 <- 2 * mod.2$coefficients
Lenths_method(eff.2, 0.2)
eff.2 <- 2 * mod.2$coefficients[-1]
Lenths_method(eff.2, 0.2)
mod.2
anova(mod.2)
# Subset a data frame for box-cox
df.input <- df %>% select(c(A,B, C, D,E,F,G,Block, Y))
df.input
output <- get_best_lambda(df.input, 0, 1.5)
# Data wrangling for plotting
tmp <- data.frame(lambda = output$lambda.range, SSR = output$SSR)
p1 <- tmp %>%
ggplot(aes(x = lambda, y = SSR)) +
geom_point() +
geom_line() +
theme_minimal() +
xlab(expression(lambda)) +
ylab("Sum of squared residuals") +
geom_vline(xintercept = output$lambda.optimal, color = "blue") +
ggtitle("Finding optimal [lambda]")
p1
output
model.followup <- lm(Y ~ ,, data = output$df.transformed)
model.followup <- lm(Y ~ ., data = output$df.transformed)
anova(model.followup)
Lenths_method(2*model.followup$coefficients[-1], 0.1)
Dongs_method(2*model.followup$coefficients[-1], 0.1)
library(tidyverse)
# Data Generating ---------------------------------------------------------
set.seed(850)
A = rep(rep(c(-1,1),each = 1),time = 16)
B = rep(rep(c(-1,1),each = 2),time = 8)
C = rep(rep(c(-1,1),each = 4),time = 4)
D = rep(rep(c(-1,1),each = 8),time = 2)
E = rep(rep(c(-1,1),each = 16),time = 1)
y = A * 0.5 + A*B * 0.6 - C*D*E*0.7 + rnorm(32,mean = 2,sd = 0.7)
# 4sigma^2/N = 0.06125
dat = data.frame(y,A,B,C,D,E)
fit = lm(y~(.)^3,data = dat)
summary(fit) # The result is quite promising
fit = lm(y~(.)^5,data = dat) # Fit the full model
# Step1: Select the one with median ~= 0 ----------------------------------
k = 5 # input the # of main effects in your design
coef_user <- function(num){ # generate the coefficient
x <- as.integer(rev(intToBits(num))) %>%
paste(collapse = "") %>%
substr(.,start = 32-k + 1,stop = 32) #
res = sapply(1:k,function(t) as.numeric(substr(x,t,t))*2-1)
return(res)
}
perm = sapply(0:(2^k-1),coef_user)
perm
coef_user()
coef_user(1)
coef_user(2)
coef_user(3)
coef_user(5)
